{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647aa335",
   "metadata": {},
   "source": [
    "# Master Project: Research Agent with LangGraph, GPT-4o, RAG, Pinecone, ArXiv and Google SerpAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e1eb6",
   "metadata": {},
   "source": [
    "## 01 - Extracting Data from ArXiv into a Pandas DataFrame and Saving it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Namespace for ArXiv's Atom-based XML format.\n",
    "ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n",
    "\n",
    "def extract_from_arxiv(search_query='cat:cs.AI', max_results=100, json_file_path='files/arxiv_dataset.json'):\n",
    "    \"\"\"\n",
    "    Fetches papers from the ArXiv API based on a search query, saves them as JSON, \n",
    "    and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The search query for ArXiv (default is 'cat:cs.AI').\n",
    "        max_results (int): The maximum number of results to retrieve (default is 100).\n",
    "        json_file_path (str): File path where JSON data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted paper information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the API request.\n",
    "    url = f'http://export.arxiv.org/api/query?search_query={search_query}&max_results={max_results}'\n",
    "    \n",
    "    # Send a GET request to the ArXiv API.\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the XML response.\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    # Loop through each \"entry\" in the XML, representing a single paper.\n",
    "    for entry in root.findall(f'{ARXIV_NAMESPACE}entry'):\n",
    "        title = entry.find(f'{ARXIV_NAMESPACE}title').text.strip()\n",
    "        summary = entry.find(f'{ARXIV_NAMESPACE}summary').text.strip()\n",
    "\n",
    "        # Get the authors of the paper.\n",
    "        author_elements = entry.findall(f'{ARXIV_NAMESPACE}author')\n",
    "        authors = [author.find(f'{ARXIV_NAMESPACE}name').text for author in author_elements]\n",
    "\n",
    "        # Get the paper's URL.\n",
    "        paper_url = entry.find(f'{ARXIV_NAMESPACE}id').text\n",
    "        arxiv_id = paper_url.split('/')[-1]\n",
    "\n",
    "        # Check for the PDF link.\n",
    "        pdf_link = next((link.attrib['href'] for link in entry.findall(f'{ARXIV_NAMESPACE}link') \n",
    "                         if link.attrib.get('title') == 'pdf'), None)\n",
    "\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'authors': authors,\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'url': paper_url,\n",
    "            'pdf_link': pdf_link\n",
    "        })\n",
    "    \n",
    "    # Convert list into a pandas DataFrame.\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # Save the DataFrame to a JSON file.\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "        print(f'Data saved to {json_file_path} ...')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_from_arxiv(max_results=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb752ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = 'files/arxiv_dataset.json'\n",
    "with open(file_name, 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89cf00",
   "metadata": {},
   "source": [
    "## 02 - Downloading the Research Papers (PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdfs(df, download_folder='files'):\n",
    "    \"\"\"\n",
    "    Downloads PDFs from URLs listed in the DataFrame and saves them to a specified folder. \n",
    "    The file names are stored in a new column 'pdf_file_name' in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'pdf_link' column with URLs to download.\n",
    "        download_folder (str): Path to the folder where PDFs will be saved (default is 'files').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional 'pdf_file_name' column containing \n",
    "                      the paths of the downloaded PDF files or None if the download failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    pdf_file_names = []\n",
    "    \n",
    "    # Loop through each row to download PDFs\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_link = row['pdf_link']\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "    \n",
    "            file_name = os.path.join(download_folder, pdf_link.split('/')[-1]) + '.pdf'\n",
    "            pdf_file_names.append(file_name)\n",
    "    \n",
    "            # Save the downloaded PDF\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f'PDF downloaded successfully and saved as {file_name}')\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Failed to download the PDF: {e}')\n",
    "            pdf_file_names.append(None)\n",
    "    \n",
    "    df['pdf_file_name'] = pdf_file_names\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_pdfs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473ec1b",
   "metadata": {},
   "source": [
    "## 03 - Loading and Splitting PDF Files into Chunks, Expanding the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62114d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(pdf_file_name, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Loads a PDF file and splits its content into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be loaded.\n",
    "        chunk_size (int): The maximum size of each chunk in characters (default is 512).\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Loading and splitting into chunks: {pdf_file_name}')\n",
    "\n",
    "    # Load the content of the PDF\n",
    "    loader = PyPDFLoader(pdf_file_name)\n",
    "    data = loader.load()\n",
    "\n",
    "    # Split the content into chunks with slight overlap to preserve context\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=64)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_df(df):\n",
    "    \"\"\"\n",
    "    Expands each row in the DataFrame by splitting PDF documents into chunks.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'pdf_file_name', 'arxiv_id', 'title', 'summary', \n",
    "                           'authors', and 'url' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame where each row represents a chunk of the original document, \n",
    "                      with additional metadata such as chunk identifiers and relationships to \n",
    "                      adjacent chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_rows = []  # List to store expanded rows with chunk information\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            chunks = load_and_chunk_pdf(row['pdf_file_name'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['pdf_file_name']}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Loop over the chunks and construct a new DataFrame row for each\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prechunk_id = i-1 if i > 0 else ''  # Preceding chunk ID\n",
    "            postchunk_id = i+1 if i < len(chunks) - 1 else ''  # Following chunk ID\n",
    "\n",
    "            expanded_rows.append({\n",
    "                'id': f\"{row['arxiv_id']}#{i}\",  # Unique chunk identifier\n",
    "                'title': row['title'],\n",
    "                'summary': row['summary'],\n",
    "                'authors': row['authors'],\n",
    "                'arxiv_id': row['arxiv_id'],\n",
    "                'url': row['url'],\n",
    "                'chunk': chunk.page_content,  # Text content of the chunk\n",
    "                'prechunk_id': '' if i == 0 else f\"{row['arxiv_id']}#{prechunk_id}\",  # Previous chunk ID\n",
    "                'postchunk_id': '' if i == len(chunks) - 1 else f\"{row['arxiv_id']}#{postchunk_id}\"  # Next chunk ID\n",
    "            })\n",
    "\n",
    "    # Return a new expanded DataFrame\n",
    "    return pd.DataFrame(expanded_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68574eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expand_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f17c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566687fe",
   "metadata": {},
   "source": [
    "## 04 - Building a Knowledge Base for the RAG System Using Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "# Check if 'OPENAI_API_KEY' is set; prompt if not\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY') or getpass('OpenAI API key: ')\n",
    "\n",
    "# Initialize the OpenAIEncoder with a specific model\n",
    "encoder = OpenAIEncoder(name='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder('hello hallo hola salut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = len(encoder('hello hallo hola salut')[0])\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24e09a",
   "metadata": {},
   "source": [
    "## 05 - Creating a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Check if 'PINECONE_API_KEY' is set; prompt if not\n",
    "api_key = os.getenv('PINECONE_API_KEY') or getpass('Pinecone API key: ')\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# Define the serverless specification for Pinecone (AWS region 'us-east-1')\n",
    "spec = ServerlessSpec(\n",
    "    cloud='aws', \n",
    "    region='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd07949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the name of the index\n",
    "index_name = 'langgraph-research-agent'\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # Embedding dimension (1536)\n",
    "        metric='cosine',\n",
    "        spec=spec  # Cloud provider and region specification\n",
    "    )\n",
    "\n",
    "    # Wait until the index is fully initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Add a short delay before checking the stats\n",
    "time.sleep(1)\n",
    "\n",
    "# View the index statistics\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075eb817",
   "metadata": {},
   "source": [
    "## 06 - Populating the Knowledge Base and Uploading it to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193524f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data = expanded_df\n",
    "batch_size = 64  # Set batch size\n",
    "\n",
    "# Loop through the data in batches, using tqdm for a progress bar\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)  # Define batch endpoint\n",
    "    batch = data[i:i_end].to_dict(orient='records')  # Slice data into a batch\n",
    "\n",
    "    # Extract metadata for each chunk in the batch\n",
    "    metadata = [{\n",
    "        'arxiv_id': r['arxiv_id'],\n",
    "        'title': r['title'],\n",
    "        'chunk': r['chunk'],\n",
    "    } for r in batch]\n",
    "    \n",
    "    # Generate unique IDs for each chunk\n",
    "    ids = [r['id'] for r in batch]\n",
    "    \n",
    "    # Extract the chunk content\n",
    "    chunks = [r['chunk'] for r in batch]\n",
    "    \n",
    "    # Convert chunks into embeddings\n",
    "    embeds = encoder(chunks)\n",
    "    \n",
    "    # Upload embeddings, IDs, and metadata to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4147222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the index statistics\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650c87",
   "metadata": {},
   "source": [
    "## 07 - Implementing the ArXiv Fetch Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Specify the arXiv ID for the paper\n",
    "arxiv_id = '1706.03762'\n",
    "\n",
    "# Make a GET request to retrieve the page for the specified paper\n",
    "res = requests.get(f'https://arxiv.org/abs/{arxiv_id}')\n",
    "\n",
    "# Access the content of the response as a string (HTML)\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f562384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Compile a regular expression pattern to find the abstract in the HTML response\n",
    "abstract_pattern = re.compile(\n",
    "    r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "# Search for the abstract in the HTML response text\n",
    "re_match = abstract_pattern.search(res.text)\n",
    "\n",
    "# Check if the abstract was found and print it; otherwise, display an error message\n",
    "if re_match:\n",
    "    print(re_match.group(1))\n",
    "else:\n",
    "    print('Abstract not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Compile a regular expression pattern to find the abstract in the HTML response\n",
    "abstract_pattern = re.compile(\n",
    "    r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "@tool('fetch_arxiv')\n",
    "def fetch_arxiv(arxiv_id: str) -> str:\n",
    "    '''Fetches the abstract from an ArXiv paper given its ArXiv ID.\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The ArXiv paper ID.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted abstract text from the ArXiv paper.\n",
    "    '''\n",
    "\n",
    "    res = requests.get(f'https://arxiv.org/abs/{arxiv_id}')\n",
    "    \n",
    "    re_match = abstract_pattern.search(res.text)\n",
    "\n",
    "    return re_match.group(1) if re_match else 'Abstract not found.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ArXiv paper ID and invoking the tool with that ID.\n",
    "arxiv_id = '1706.03762'\n",
    "output = fetch_arxiv.invoke(input={'arxiv_id': arxiv_id})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42bd96",
   "metadata": {},
   "source": [
    "## Integrating Google SerpAPI for Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the API keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ca3fd",
   "metadata": {},
   "source": [
    "### Implementing the Web Search Tools with Google SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4844c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up the SerpAPI request parameters, including the API key.\n",
    "serpapi_params = {\n",
    "    'engine': 'google',  \n",
    "    'api_key': os.getenv('SERPAPI_KEY') or getpass('SerpAPI key: ')  # Get the API key securely.\n",
    "}\n",
    "\n",
    "# Perform a Google search for the keyword \"water\" and limit the results to 5.\n",
    "search = GoogleSearch({\n",
    "    **serpapi_params,\n",
    "    'q': 'water',\n",
    "    'num': 5\n",
    "})\n",
    "\n",
    "\n",
    "# Extract the main search results from the API response.\n",
    "results = search.get_dict().get('organic_results', [])\n",
    "\n",
    "# Format the search results for readability.\n",
    "formatted_results = '\\n---\\n'.join(\n",
    "    ['\\n'.join([x['title'], x['snippet'], x['link']]) for x in results]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec38ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c385f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define the 'web_search' tool using the '@tool' decorator.\n",
    "@tool('web_search')\n",
    "def web_search(query: str) -> str:\n",
    "    '''Finds general knowledge information using a Google search.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of the top search results, including title, snippet, and link.\n",
    "    '''\n",
    "\n",
    "    search = GoogleSearch({\n",
    "        **serpapi_params,  \n",
    "        'q': query,        \n",
    "        'num': 5         \n",
    "    })\n",
    "   \n",
    "    results = search.get_dict().get('organic_results', [])\n",
    "    formatted_results = '\\n---\\n'.join(\n",
    "        ['\\n'.join([x['title'], x['snippet'], x['link']]) for x in results]\n",
    "    )\n",
    "    \n",
    "    # Return the formatted results or a 'No results found.' message if no results exist.\n",
    "    return formatted_results if results else 'No results found.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the 'web_search' tool with the query 'water on mars'\n",
    "output = web_search.invoke(input={'query': 'water on mars'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b8202",
   "metadata": {},
   "source": [
    "## 09 - Creating RAG Tools for Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c218b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_contexts(matches: list) -> str:\n",
    "    '''Formats the retrieved context matches into a readable string format.\n",
    "\n",
    "    Args:\n",
    "        matches (list): A list of matched documents with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of document titles, chunks, and ArXiv IDs.\n",
    "    '''\n",
    "    formatted_results = []\n",
    "    \n",
    "    # Loop through each match and extract its metadata.\n",
    "    for x in matches:\n",
    "        text = (\n",
    "            f\"Title: {x['metadata']['title']}\\n\"\n",
    "            f\"Chunk: {x['metadata']['chunk']}\\n\"\n",
    "            f\"ArXiv ID: {x['metadata']['arxiv_id']}\\n\"\n",
    "        )\n",
    "        # Append each formatted string to the results list.\n",
    "        formatted_results.append(text)\n",
    "    \n",
    "    # Join all the individual formatted strings into one large string.\n",
    "    return '\\n---\\n'.join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def rag_search_filter(query: str, arxiv_id: str) -> str:\n",
    "    '''Finds information from the ArXiv database using a natural language query and a specific ArXiv ID.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query in natural language.\n",
    "        arxiv_id (str): The ArXiv ID of the specific paper to filter by.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of relevant document contexts.\n",
    "    '''\n",
    "    \n",
    "    # Encode the query into a vector representation.\n",
    "    xq = encoder([query])\n",
    "    \n",
    "    # Perform a search on the Pinecone index, filtering by ArXiv ID.\n",
    "    xc = index.query(vector=xq, top_k=6, include_metadata=True, filter={'arxiv_id': arxiv_id})\n",
    "    \n",
    "    # Format and return the search results.\n",
    "    return format_rag_contexts(xc['matches'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool('rag_search')\n",
    "def rag_search(query: str) -> str:\n",
    "    '''Finds specialist information on AI using a natural language query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query in natural language.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of relevant document contexts.\n",
    "    '''\n",
    "    \n",
    "    # Encode the query into a vector representation.\n",
    "    xq = encoder([query])\n",
    "    \n",
    "    # Perform a broader search without filtering by ArXiv ID.\n",
    "    xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "    \n",
    "    # Format and return the search results.\n",
    "    return format_rag_contexts(xc['matches'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893af88",
   "metadata": {},
   "source": [
    "## 10 - Implementing the Final Answer Generation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define the 'final_answer' tool to compile the research report.\n",
    "@tool\n",
    "def final_answer(\n",
    "    introduction: str,\n",
    "    research_steps: str or list,\n",
    "    main_body: str,\n",
    "    conclusion: str,\n",
    "    sources: str or list\n",
    ") -> str:\n",
    "    '''Returns a natural language response in the form of a research report.\n",
    "\n",
    "    Args:\n",
    "        introduction (str): A short paragraph introducing the user's question and the topic.\n",
    "        research_steps (str or list): Bullet points or text explaining the steps taken for research.\n",
    "        main_body (str): The bulk of the answer, 3-4 paragraphs long, providing high-quality information.\n",
    "        conclusion (str): A short paragraph summarizing the findings.\n",
    "        sources (str or list): A list or text providing the sources referenced during the research.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted research report string.\n",
    "    '''\n",
    "\n",
    "    # Format research steps if given as a list.\n",
    "    if isinstance(research_steps, list):\n",
    "        research_steps = '\\n'.join([f'- {r}' for r in research_steps])\n",
    "    \n",
    "    # Format sources if given as a list.\n",
    "    if isinstance(sources, list):\n",
    "        sources = '\\n'.join([f'- {s}' for s in sources])\n",
    "    \n",
    "    # Construct and return the final research report.\n",
    "    return f'{introduction}\\n\\nResearch Steps:\\n{research_steps}\\n\\nMain Body:\\n{main_body}\\n\\n \\\n",
    "    Conclusion:\\n{conclusion}\\n\\nSources:\\n{sources}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba05dde",
   "metadata": {},
   "source": [
    "## 11 - Initializing the \"Oracle\" LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c405efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define the system prompt guiding the AI's decision-making process.\n",
    "system_prompt = (\n",
    "    '''You are the oracle, the great AI decision-maker.\n",
    "    Given the user's query, you must decide what to do with it based on the\n",
    "    list of tools provided to you.\n",
    "\n",
    "    If you see that a tool has been used (in the scratchpad) with a particular\n",
    "    query, do NOT use that same tool with the same query again. Also, do NOT use\n",
    "    any tool more than twice (i.e., if the tool appears in the scratchpad twice, do\n",
    "    not use it again).\n",
    "\n",
    "    You should aim to collect information from a diverse range of sources before\n",
    "    providing the answer to the user. Once you have collected plenty of information\n",
    "    to answer the user's question (stored in the scratchpad), use the final_answer tool.'''\n",
    ")\n",
    "\n",
    "\n",
    "# Create a prompt template for the conversation flow.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),  # Define the AI's role and rules.\n",
    "    \n",
    "    # Insert past chat messages to maintain context.\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    \n",
    "    # Insert user's input dynamically.\n",
    "    ('user', '{input}'),\n",
    "    \n",
    "    # Include the assistant's scratchpad to track tool usage and intermediate steps.\n",
    "    ('assistant', 'scratchpad: {scratchpad}'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7991e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolCall, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI language model with specific settings.\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define the list of tools available to the oracle.\n",
    "tools = [\n",
    "    rag_search_filter,\n",
    "    rag_search,\n",
    "    fetch_arxiv,\n",
    "    web_search,\n",
    "    final_answer\n",
    "]\n",
    "\n",
    "# Function to create the scratchpad from the intermediate tool calls.\n",
    "def create_scratchpad(intermediate_steps: list[ToolCall]) -> str:\n",
    "    research_steps = []\n",
    "    \n",
    "    # Loop over each step and process tool calls with actual outputs.\n",
    "    for i, action in enumerate(intermediate_steps):\n",
    "        if action.log != 'TBD':\n",
    "            research_steps.append(\n",
    "                f'Tool: {action.tool}, input: {action.tool_input}\\n'\n",
    "                f'Output: {action.log}'\n",
    "            )\n",
    "    \n",
    "    # Join the research steps into a readable log.\n",
    "    return '\\n---\\n'.join(research_steps)\n",
    "\n",
    "# Define the oracle's decision-making pipeline.\n",
    "oracle = (\n",
    "    {\n",
    "        'input': lambda x: x['input'],\n",
    "        'chat_history': lambda x: x['chat_history'],\n",
    "        'scratchpad': lambda x: create_scratchpad(intermediate_steps=x['intermediate_steps']),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice='any')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d542f",
   "metadata": {},
   "source": [
    "## 12 -  Testing the Oracle and the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = 'Tell me something interesting about dynamic backtracking AI and LLMs'\n",
    "# input = 'Who won the Super Bowl 2024?'\n",
    "input = 'What is the ArXiv paper with the ID 2407.21783 all about?'\n",
    "# Create the inputs dictionary, containing the user's query and initial empty chat history and intermediate steps.\n",
    "inputs = {\n",
    "    'input': input,\n",
    "    'chat_history': [],\n",
    "    'intermediate_steps': [],\n",
    "}\n",
    "\n",
    "# Invoke the oracle with the inputs, processing the query and returning a response.\n",
    "out = oracle.invoke(inputs)\n",
    "\n",
    "# Display the oracle's response.\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the name of the tool\n",
    "out.tool_calls[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tool's arguments\n",
    "out.tool_calls[0]['args']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae93bf5",
   "metadata": {},
   "source": [
    "##  Building a Decision-Making Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c09f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_oracle(): main function that executes the oracle and processes its output to extract the relevant tool and its arguments.\n",
    "# We'll use this information to update the state for future steps.\n",
    "def run_oracle(state: dict) -> dict:\n",
    "    '''Runs the oracle and processes the output to extract tool information.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state containing the 'intermediate_steps'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new state with updated 'intermediate_steps' including the tool action.\n",
    "    '''\n",
    "    \n",
    "    print('run_oracle')\n",
    "    print(f'intermediate_steps: {state[\"intermediate_steps\"]}')\n",
    "    \n",
    "    # Invoke the oracle with the current state.\n",
    "    out = oracle.invoke(state)\n",
    "\n",
    "    # Extract the tool name and its arguments from the oracle's response.\n",
    "    tool_name = out.tool_calls[0]['name']\n",
    "    tool_args = out.tool_calls[0]['args']\n",
    "\n",
    "    # Create an AgentAction object, which records the tool used and the input provided.\n",
    "    action_out = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log='TBD'  # To be determined later after the tool runs.\n",
    "    )\n",
    "\n",
    "    # Return a new state with updated 'intermediate_steps'.\n",
    "    return {\n",
    "        'intermediate_steps': [action_out]\n",
    "    }\n",
    "\n",
    "\n",
    "# The router() function determines the next tool to use based on the current state.\n",
    "def router(state: dict) -> str:\n",
    "    '''Determines the next tool to use based on the current state.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state containing 'intermediate_steps'.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the tool to use next.\n",
    "    '''\n",
    "\n",
    "    if isinstance(state['intermediate_steps'], list):\n",
    "        return state['intermediate_steps'][-1].tool\n",
    "    else:\n",
    "        print('Router invalid format')\n",
    "        return 'final_answer'\n",
    "\n",
    "\n",
    "tool_str_to_func = {\n",
    "    'rag_search_filter': rag_search_filter,\n",
    "    'rag_search': rag_search,\n",
    "    'fetch_arxiv': fetch_arxiv,\n",
    "    'web_search': web_search,\n",
    "    'final_answer': final_answer\n",
    "}\n",
    "\n",
    "# The run_tool() function executes the appropriate tool based on the current state.\n",
    "def run_tool(state: dict) -> dict:\n",
    "    '''Executes the appropriate tool based on the current state.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state containing the 'intermediate_steps'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new state with updated 'intermediate_steps' including the tool's result.\n",
    "    '''\n",
    "\n",
    "    tool_name = state['intermediate_steps'][-1].tool\n",
    "    tool_args = state['intermediate_steps'][-1].tool_input\n",
    "\n",
    "    print(f'{tool_name}.invoke(input={tool_args})')\n",
    "\n",
    "    out = tool_str_to_func[tool_name].invoke(input=tool_args)\n",
    "\n",
    "    action_out = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=str(out)\n",
    "    )\n",
    "\n",
    "    return {'intermediate_steps': [action_out]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b176fe",
   "metadata": {},
   "source": [
    "## 13 - Defining the Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    '''Represents the state of an agent.'''\n",
    "    \n",
    "    input: str\n",
    "    chat_history: List[BaseMessage]\n",
    "    intermediate_steps: Annotated[List[tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfff22",
   "metadata": {},
   "source": [
    "## 14 - Defining the Graph for Decision-Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize the state graph with AgentState to manage the workflow.\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node('oracle', run_oracle)\n",
    "graph.add_node('rag_search_filter', run_tool)\n",
    "graph.add_node('rag_search', run_tool)\n",
    "graph.add_node('fetch_arxiv', run_tool)\n",
    "graph.add_node('web_search', run_tool)\n",
    "graph.add_node('final_answer', run_tool)\n",
    "\n",
    "# Set the entry point to 'oracle'.\n",
    "graph.set_entry_point('oracle')\n",
    "\n",
    "# Add conditional edges to determine the next step using the router function.\n",
    "graph.add_conditional_edges(source='oracle', path=router)\n",
    "\n",
    "# Add edges from each tool back to 'oracle', except 'final_answer', which leads to 'END'.\n",
    "for tool_obj in tools:\n",
    "    if tool_obj.name != 'final_answer':\n",
    "        graph.add_edge(tool_obj.name, 'oracle')\n",
    "\n",
    "graph.add_edge('final_answer', END)\n",
    "\n",
    "# Compile the graph to make it executable.\n",
    "runnable = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the graph as a PNG using Mermaid rendering.\n",
    "display(Image(runnable.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470299b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph with input.\n",
    "output = runnable.invoke({\n",
    "    'input': 'Tell me something interesting about Dynamic Backtracking AI and LLMs',\n",
    "    'chat_history': [],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eb72b",
   "metadata": {},
   "source": [
    "## Building a Formatted Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_report(output: dict) -> str:\n",
    "    '''Builds a formatted report based on the oracle's output.\n",
    "\n",
    "    Args:\n",
    "        output (dict): A dictionary containing the various sections of the report (graph's output).\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the full research report.\n",
    "    '''\n",
    "    research_steps = output['research_steps']\n",
    "    if isinstance(research_steps, list):\n",
    "        research_steps = '\\n'.join([f'- {r}' for r in research_steps])\n",
    "    \n",
    "    sources = output['sources']\n",
    "    if isinstance(sources, list):\n",
    "        sources = '\\n'.join([f'- {s}' for s in sources])\n",
    "    \n",
    "    return f\"\"\"\n",
    "        INTRODUCTION\n",
    "        ------------\n",
    "        {output['introduction']}\n",
    "        \n",
    "        RESEARCH STEPS\n",
    "        --------------\n",
    "        {research_steps}\n",
    "        \n",
    "        REPORT\n",
    "        ------\n",
    "        {output['main_body']}\n",
    "        \n",
    "        CONCLUSION\n",
    "        ----------\n",
    "        {output['conclusion']}\n",
    "        \n",
    "        SOURCES\n",
    "        -------\n",
    "        {sources}\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = build_report(\n",
    "    output=output['intermediate_steps'][-1].tool_input\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = runnable.invoke({\n",
    "    'input': 'What is the potential of AI in fintech companies?',\n",
    "    'chat_history': []  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = build_report(\n",
    "    output=output['intermediate_steps'][-1].tool_input\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc830976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
