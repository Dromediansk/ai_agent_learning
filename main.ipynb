{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c3ed25",
   "metadata": {},
   "source": [
    "# Project: Question-Answering on Private Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbb6c6-c36f-400f-af22-f6d19ae7b0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dotenv pypdf langchain langchain-community docx2txt wikipedia tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b2a93-5674-4702-b952-6111b5a4a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9f229-56e2-49c0-af58-34ee8ec3607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "\n",
    "    elif extension == '.docx':\n",
    "            from langchain.document_loaders import Docx2txtLoader\n",
    "            print(f'Loading {file}')\n",
    "            loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file type: {extension}')\n",
    "\n",
    "    return loader.load()\n",
    "\n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    print(f'Loading {query} from Wikipedia')\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef259d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "  chunks = text_splitter.split_documents(data)\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d517248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "  import tiktoken\n",
    "  enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "  total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "  print(f\"Total embedding tokens: {total_tokens}\")\n",
    "  print(f\"Total embedding cost (at $0.0004 / 1K tokens): ${total_tokens * 0.0004 / 1000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4938afe",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a  Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai langchain-openai pinecone langchain-pinecone -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58277466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "  import pinecone\n",
    "  from langchain_pinecone import PineconeVectorStore  # Updated import for new Pinecone API\n",
    "  from langchain_openai import OpenAIEmbeddings\n",
    "  from pinecone import ServerlessSpec\n",
    "\n",
    "  pc = pinecone.Pinecone()\n",
    "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "\n",
    "  if index_name in pc.list_indexes().names():\n",
    "    print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "    vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    print('Done')\n",
    "    return vector_store\n",
    "  else:\n",
    "    print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "    pc.create_index(\n",
    "      name=index_name, \n",
    "      dimension=1536, \n",
    "      metric='cosine', \n",
    "      spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "      documents=chunks, \n",
    "      embedding=embeddings, \n",
    "      index_name=index_name\n",
    "    )\n",
    "    print('Done')\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name = 'all'):\n",
    "  import pinecone\n",
    "  pc = pinecone.Pinecone()\n",
    "\n",
    "  if index_name == 'all':\n",
    "    indexes = pc.list_indexes().names()\n",
    "    print(f'Deleting all indexes: {indexes}')\n",
    "    for index in indexes:\n",
    "      pc.delete_index(index)\n",
    "    print('Done')\n",
    "  else:\n",
    "    print(f'Deleting index: {index_name} ...', end='')\n",
    "    pc.delete_index(index_name)\n",
    "    print('Done')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd95f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "You have 41 pages in your data\n",
      "There are 639 characters in the first page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/us_constitution.pdf')\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[0].page_content)} characters in the first page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_document('files/the_great_gatsby.docx')\n",
    "# print(f'You have {len(data)} pages in your data')\n",
    "# print(f'There are {len(data[0].page_content)} characters in the first page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef84423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_from_wikipedia('GPT-5', 'sk')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee04d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 224 chunks of data\n",
      "The United States Constitution \n",
      " W e the People of the United States, in Order to form a more perfect \n",
      " Union, establish Justice, insure domestic T ranquility , provide for the \n",
      " common defence, promote the general W elfare, and secure the\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(f'Now you have {len(chunks)} chunks of data')\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miroslav/Documents/repos/qa_document_app/.venv/lib/python3.12/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index askadocument already exists. Loading embeddings ...Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43240ff5",
   "metadata": {},
   "source": [
    "### Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83192d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "  from langchain.chains import RetrievalQA\n",
    "  from langchain_openai import ChatOpenAI\n",
    "\n",
    "  llm = ChatOpenAI(model='gpt-4o', temperature=1)\n",
    "\n",
    "  retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "\n",
    "  chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "  answer = chain.invoke(q)\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides information about ChatGPT, a language model developed by OpenAI. It mentions that ChatGPT is a variant of the GPT model and is used in conversational robots and natural language processing applications. Additionally, it discusses the importance of registration for usage, the initial testing phase aimed at identifying strengths and weaknesses, and collecting user feedback for improvements. The document also references related topics like internet bots and offers links to external resources related to chatbots.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c650e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n",
      "Exiting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "\n",
    "while True:\n",
    "  q = input('Question #{i}: ')\n",
    "  i += 1\n",
    "  if q.lower() in ['quit', 'exit']:\n",
    "    print('Exiting ... bye bye!')\n",
    "    time.sleep(2)\n",
    "    break\n",
    "\n",
    "  answer = ask_and_get_answer(vector_store, q)\n",
    "  print(f'\\nAnswer: {answer['result']}')\n",
    "  print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChatGPT from Wikipedia\n",
      "Index chatgpt already exists. Loading embeddings ...Done\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'sk')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359b1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neviem, či bol GPT-5 spustený, pretože vo vašom kontexte sa neuvádzajú žiadne informácie o GPT-5.\n"
     ]
    }
   ],
   "source": [
    "q = 'Kedy bol GPT 5 spustený?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86efed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes: ['chatgpt', 'askadocument']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9e259",
   "metadata": {},
   "source": [
    "### Using Chroma as a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c79538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cc2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='.chroma_db'):\n",
    "  from langchain.vectorstores import Chroma\n",
    "  from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "  vector_store = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\n",
    "  return vector_store\n",
    "\n",
    "def load_embeddings_chroma(persist_directory='.chroma_db'):\n",
    "  from langchain.vectorstores import Chroma\n",
    "  from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "  vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "  return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfc9ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "996c0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Search is a fully-managed platform by Google that enables developers to build AI-powered search applications. It offers capabilities such as customizable answers, search tuning, vector search, grounding, and compliance updates. Vertex AI Search leverages Tensor Processing Units (TPUs) to power large-scale semantic searches with Google-quality performance, providing developers with the tools to create and enhance AI applications without the need to design and build their own advanced search engines.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is Vertext AI Search?'\n",
    "\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0021273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Stack Overflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "\n",
    "answer = ask_and_get_answer(db, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "463e5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I need a specific number to multiply by 2. Could you please provide the number you'd like to multiply?\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(db, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5c6",
   "metadata": {},
   "source": [
    "### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fcd48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/06x6xl311qlgvlq77ry8fz5m0000gn/T/ipykernel_57392/2784899899.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69daab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "  result = chain.invoke({\"question\": q})\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6e8d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eb53026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4502b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed24f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Follow Up Input: Multiply that number by 2.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Human: What is the result when you multiply the 8 million pairs of questions and answers in the StackOverflow dataset by 2?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 2.'\n",
    "result = ask_question(q, crc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b1dd1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you multiply the 8 million pairs of questions and answers by 2, you get 16 million pairs.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7ac1a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?' additional_kwargs={} response_metadata={}\n",
      "content='The StackOverflow dataset had 8 million pairs of questions and answers.' additional_kwargs={} response_metadata={}\n",
      "content='Multiply that number by 2.' additional_kwargs={} response_metadata={}\n",
      "content='When you multiply the 8 million pairs of questions and answers by 2, you get 16 million pairs.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dcd7b",
   "metadata": {},
   "source": [
    "### Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question in Spanish.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know\".\n",
    "-------------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = r'''\n",
    "Question: ```{question}```\n",
    "Chat History: ```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e31506",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_embeddings_chroma()\n",
    "\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab35d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
